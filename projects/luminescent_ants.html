<!DOCTYPE html>
<html class="no-js" lang="en">
<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="UTF-8">
    <title>luminescent ants</title>
   <!-- Description Metadata-->
    <meta name="description" content="Personal Website" />
    <!-- Author Metadata -->
    <meta name="author" content="Mario Jerez" />
    <!-- Keyword Metadata -->
    <meta
      name="keywords"
      content="Mario Jerez, Mario E Jerez, Mario, Jerez, luminescent ants"
    />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="../assets/css/luminescent_ants.css">

    <!-- script
    ================================================== -->
    <script defer src="../assets/js/all.min.js"></script>

    <!-- favicons
    ================================================== -->
    <!-- <link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="manifest" href="site.webmanifest"> -->

</head>

<body id="top" class="ss-preload">


    <!-- preloader
    ================================================== -->
    <div id="preloader">
        <div id="loader"></div>
    </div>


    <!-- header
    ================================================== -->
 

    
    <!-- hero
    ================================================== -->
    <section id="hero" class="s-hero target-section">
        
        <div class="s-hero__bg rellax" data-rellax-speed="-7"></div>

        <div class="row s-hero__content">
            <div class="column">

                <div class="s-hero__content-about">

                    <h1>Luminescent Ant Colony</h1>
                    <h2>Intelligent Robotic Systems (CSCI 5551) | Final Project</h2>
                    <h3>Mario Jerez</h3>
                </div> <!-- end s-hero__content-about -->

            </div>
        </div> <!-- s-hero__content -->
    </section> <!-- end s-hero -->    

    <section class="action-section">
    <ul class="actions">
        <li><a href="../index.html" class="button">Home</a></li>
        <li><a href="https://github.com/mariojerez/luminescent-ants" target="_blank" class="button">Github Code</a></li>
        <!-- <li><a href="../publication.html" class="button">Publications</a></li> -->
    </ul>
    </section>



<!-- Project Motivation and Background -->
<section id="project-background" class="section-background">
    <div class="row">
        <div class="column large-12">
            <div class="content-block">
                <h2>Project Motivation and Background</h2>
                <h3>Recreating Foraging Behavior with Robots</h3>
                <p>Much research has been done to attempt to emulate the behaviors of social insects found in nature. The ability of ant colonies to find the most optimal paths between the nest and resources has inspired exciting probabilistic techniques known as Ant Colony Optimization Algorithms (ACO). Applications for ACO include solving complex problems such as Traveling Salesman, Vehicle Routing, Project Scheduling, and Protein Folding problems. However, there is still a lot of progress to be made in the most obvious application of it: foraging for resources in the real world. The potential applications for a framework that allows robots to cooperate in a decentralized way are numerous. It could be used to explore new environments, such as different planets or oceans, where the robot system would have to be trustworthy enough to make good decisions with minimal human oversight. This paper seeks to explore and build on techniques that have the potential to be developed into robust, error-tolerant, and scalable frameworks for useful, decentralized collaboration between robots.</p>
           
            </div>
        </div>
    </div>
    <div class="row">
        <div class="column large-12">
            <div class="content-block">
                <h3>Project Focus</h3>
                <p>This project is centered around the application and exploration of ORB-SLAM3, a SLAM/visual odometry system, within an robotics context.</p>
            </div>
        </div>
    </div>
</section>





<section id="vo-methods" class="vo-methods-section">
    <div class="container">
        <h2>Methods for Visual Odometry</h2>

        <div class="vo-methods-content">
            <!-- <img src="./Images_Spot/VO_methods.png" alt="Visual Odometry Methods" class="vo-methods-image"> -->

            <div class="vo-methods-description">
                <h3>Direct Methods</h3>
                <p>Direct methods of visual odometry work by directly using pixel intensity values from images to estimate motion. This approach does not explicitly extract features from the images. Instead, it utilizes the raw image data to compute the motion between frames.</p>
                <p>
                    In direct methods of visual odometry, pixel intensity matching is used to estimate motion by comparing pixel intensities between successive images. This approach is particularly robust in low-texture environments where extracting features might be challenging, although it can be sensitive to lighting changes and might require more computational resources. An example of this method is the Semi-direct Visual Odometry (SVO), which uniquely combines feature-based and direct approaches.
                </p>

                <h3>Indirect Methods</h3>
                <p>Indirect methods, also known as feature-based methods, involve detecting and matching key features between different images. These features could be points, edges, or other distinct elements in the environment.</p>
                <p>
                    Indirect methods, or feature-based methods, involve extracting distinct features like points or edges from images, and then matching these features across different frames to estimate motion. This approach tends to be more robust against changes in lighting and is generally more efficient, but it might face challenges in environments with few distinct features or in highly dynamic scenes. ORB-SLAM, which utilizes Oriented FAST and Rotated BRIEF features, is a widely-used example of an indirect method.
                </p>

                <h3>Hybrid Methods</h3>
                <p>In addition to purely direct or indirect approaches, there are hybrid methods that combine elements of both. These methods aim to leverage the strengths of each approach to improve accuracy and robustness.</p>
                <p>
                    Hybrid methods in visual odometry, such as Semi-direct Visual Odometry (SVO), blend the advantages of both feature extraction and pixel intensity analysis. This combination allows for accurate feature matching and enhanced motion estimation by directly using pixel data. SVO is a prime example of such a hybrid approach, offering a sophisticated balance of direct and indirect visual odometry techniques. For more insights into SVO, visit <a href="https://rpg.ifi.uzh.ch/svo_pro.html">Semi-direct Visual Odometry (SVO) at the University of Zurich</a>.
                </p>
                
            </div>
        </div>
    </div>
</section>


<section id="orb-slam-overview" class="slam-overview">
    <div class="container">
        <h2>Overview of ORB-SLAM Developments</h2>
        <p>
            <a href="https://arxiv.org/pdf/1610.06475.pdf" target="_blank" class="paper-link">
               ORB-SLAM3
            </a> delves into the latest advancements in Visual SLAM and VO, highlighting the role of loop closing techniques in merging the domains of VO and SLAM. It underscores the importance of MAP estimation and BA in both geometric and photometric contexts. This research extends beyond simple ego-motion tracking, focusing on creating and using comprehensive environmental maps. It details three types of data associations—short-term, mid-term, and long-term—and builds upon the foundations set by ORB-SLAM and ORB-SLAM Visual-Inertial. The paper brings to light the concept of multi-map data association, aiming to refine map accuracy and usability, a core objective of SLAM systems.
        </p>
        <p>
            This work is grounded in the principles established by two key papers: ORBSLAM 2 and ORB SLAM VI (Visual-Inertial Monocular SLAM with Map Reuse), which have been instrumental in advancing SLAM technologies in varied environments.
        </p>
    </div>
</section>


<section id="paper-links" class="papers-section">
    <div class="container">
        <div class="papers-container">
            <a href="https://arxiv.org/pdf/1610.06475.pdf" target="_blank" class="paper-link">
                <!-- <img src="./Images_Spot/ORB2.png" alt="ORB SLAM 2 Paper" class="paper-image"> -->
            </a>
            <a href="https://arxiv.org/pdf/1610.05949.pdf" target="_blank" class="paper-link">
                <!-- <img src="./Images_Spot/ORBVI.png" alt="ORB SLAM VI Paper" class="paper-image"> -->
            </a>
        </div>
    </div>
</section>



<section id="feature-extraction-intro" class="feature-section">
    <div class="container">
        <h2>Understanding Feature Extraction in ORB-SLAM</h2>
        <p>
            Feature extraction plays a pivotal role in the implementation of ORB-SLAM. It involves identifying and using key points in images to track the movement and orientation of a camera through an environment. ORB-SLAM leverages the ORB (Oriented FAST and Rotated BRIEF) algorithm for this purpose due to its efficiency and effectiveness in real-time applications. This is crucial for ORB-SLAM, which requires quick and reliable feature detection and matching to accurately map and navigate through spaces.
        </p>
    </div>
</section>

<section id="algorithm-comparison" class="algorithm-section">
    <div class="card-container">
        <!-- SIFT Card -->
        <div class="algorithm-card" id="sift-card">
            <h3>SIFT</h3>
            <p>Scale-Invariant Feature Transform, Rich feature detection, ideal for scale and rotation variations, High computational resource requirement</p>
        </div>
        <!-- SURF Card -->
        <div class="algorithm-card" id="surf-card">
            <h3>SURF</h3>
            <p>Speeded Up Robust Features, Faster than SIFT, with similar feature richness, Effective in image matching and 3D reconstruction</p>
        </div>
        <!-- ORB Card -->
        <div class="algorithm-card" id="orb-card">
            <h3>ORB</h3>
            <p>Oriented FAST and Rotated BRIEF, Fast and efficient, suitable for real-time applications, Rotation invariant and less resource-intensive</p>
        </div>
    </div>
</section>




<section id="project-setup" class="project-setup">
    <div class="container">
        <h2>Project Setup Overview</h2>
        <p>
            In this phase of the project, I initiated by setting up a test environment in Gazebo ROS. The chosen environment was an office world from the collection available at <a href="https://github.com/leonhartyao/gazebo_models_worlds_collection">Gazebo Models and Worlds Collection by leonhartyao</a>. This selection provided a realistic and complex setting, ideal for testing the capabilities of the SPOT Boston Dynamics model in Gazebo.
        </p>
        <p>
            For the integration of the SPOT model, I utilized CHAMP, an open-source framework found at <a href="https://github.com/chvmp/champ">CHAMP by chvmp</a>. CHAMP is designed for constructing quadrupedal robots and developing control algorithms, drawing inspiration from the hierarchical control implementation on the MIT Cheetah robot. This framework was crucial in setting up the SPOT model, enabling dynamic locomotion control and effective simulation.
        </p>
        <p>
            The control of the robot's movement was facilitated by <code>champ_teleop</code>, a fork of <code>teleop_twist_keyboard</code> adapted for quadruped robots, which can be found at <a href="https://github.com/chvmp/champ_teleop">champ_teleop by chvmp</a>. This software modification allowed for the control of the robot's entire body pose, including roll, pitch, and yaw.
        </p>
        <p>
            For seamless ROS integration, the Spot ROS Driver from <a href="https://github.com/chvmp/spot_ros">chvmp/spot_ros</a> (branch: gazebo) was employed. This integration enabled effective communication between the Gazebo simulation and the ROS environment.
        </p>
        <p>
            Lastly, to acquire the specific Boston Dynamics' Spot model needed for the project, I used resources from <a href="https://github.com/chvmp/robots">chvmp/robots</a>. This repository provided a range of pre-configured robots, including the Spot model, complete with instructions for running demonstrations in Gazebo.
        </p>
        <p>
            This setup formed the foundation of my project, allowing for a comprehensive simulation of the SPOT Boston Dynamics model in a realistic office environment, paving the way for further development and experimentation in autonomous robotics.
        </p>

    </div>
</section>






<section id="orb-slam-setup" class="orb-slam-setup">
    <div class="container">
        <h2>ORB-SLAM3 Setup</h2>
        <p>
            ORB-SLAM3, released on 2021, by authors Carlos Campos, Richard Elvira, Juan J. Gómez Rodríguez, José M. M. Montiel, and Juan D. Tardos, represents a significant advancement in SLAM technology. 
            The system supports monocular, stereo, and RGB-D cameras, accommodating both pin-hole and fisheye lens models. In all sensor configurations, ORB-SLAM3 competes robustly with the best systems available, offering marked improvements in accuracy.
        </p>
        <p>
            Examples provided allow for running ORB-SLAM3 on datasets like EuRoC and TUM-VI, with configurations ranging from stereo or monocular setups to those with or without IMU. The software is a continuation of the work done in ORB-SLAM2, with significant enhancements.
        </p>
        <p>
            For my project, I utilized the source code of ORB-SLAM3, available at <a href="https://github.com/UZ-SLAMLab/ORB_SLAM3">UZ-SLAMLab/ORB_SLAM3 on GitHub</a>. Integrating it into my test environment proved to be a learning experience, as it required a detailed understanding of the necessary nodes and adapting the system to fit my specific needs. This process involved not only integrating but also re-implementing parts of the ORB-SLAM3 code, particularly the tracking components. This effort was aimed at gaining a deeper insight into the workings of the ORB algorithms and enhancing my practical skills in advanced SLAM technologies.
        </p>
        
    </div>
</section>



<section id="testing-results" class="testing-results">
    <div class="container">
        <div class="test-section" id="teleop-test">
            <h3>Teleoperation Test</h3>
            <p>
                The teleoperation test was conducted using Telo Key, enabling remote control of the robot. This test demonstrated the robot's responsiveness and maneuverability under direct human control.
            </p>
            <!-- Placeholder for results or image -->
        </div>
        <div class="test-section" id="auto-nav-test">
            <h3>Autonomous Navigation Test</h3>
            <p>
                For autonomous navigation, the robot utilized the Navigation stack with A* for path planning in ROS. The test showed the robot's capability to navigate and plan paths effectively in an autonomous mode.
            </p>
            <!-- Placeholder for results or image -->
        </div>
    </div>
</section>




<section id="odometry-comparison" class="odometry-comparison">
    <div class="container">
        <h2>Actual Test Result</h2>
        <div class="odometry-result" id="actual-odometry">
            <h3>In Gazebo</h3>
            <iframe width="100%" height="615" src="https://www.youtube.com/embed/OA0VYMmMQ54" frameborder="0" allowfullscreen></iframe>
        </div>
        <div class="odometry-result" id="orb-slam3-odometry">
            <h3>Using the Actual Robot</h3>
            <!-- Placeholder for ORB-SLAM3 odometry result image or graph -->
        </div>
        <div class="video-thumbnail">
            <iframe width="100%" height="615" src="https://www.youtube.com/embed/sI4TAg9-pUE" frameborder="0" allowfullscreen></iframe>
        </div>
    </div>
</section>





    <div class="list-container">
        <!-- Paper Link Tag -->
        <!-- <span class="tag">
            <a href="https://arxiv.org/pdf/2211.00194" class="external-link">
                <i class="fas fa-file-pdf"></i> Paper
            </a>
        </span> -->
        
        <!-- arXiv Link Tag -->
        <!-- <span class="tag">
            <a href="https://arxiv.org/pdf/2211.00194" class="external-link">
                <i class="ai ai-arxiv"></i> arXiv
            </a>
        </span> -->
        
        <!-- Video Link Tag -->
        <span class="tag">
            <a href="" class="external-link">
                <i class="fab fa-youtube"></i> Video
            </a>
        </span>
        
        <!-- Code Link Tag -->
        <span class="tag">
            <a href="https://github.com/ebasatemesgen/ORB-SLAM3-Enabled-Visual-Odometry-?tab=readme-ov-file" class="external-link">
                <i class="fab fa-github"></i> Code
            </a>
        </span>
        
        <!-- Data Link Tag -->
        <span class="tag">
            <a href="https://docs.google.com/presentation/d/1IHV-s6nDzVrucY9btHt8kPf4SDN043UO4SoM1Jj9hNw/edit?usp=sharing" class="external-link">
                <i class="far fa-images"></i> Data
            </a>
        </span>
    </div>











<!-- Java Script -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/rellax/1.12.1/rellax.min.js"></script>
<script src="https://unpkg.com/swiper/swiper-bundle.min.js"></script>
<script src="../assets/js/ex_main.js"></script>
<script src="../assets/js/ex_plugins.js"></script>

</body>
</html>
